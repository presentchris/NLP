{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GRfpGaz27IWs",
        "outputId": "35284720-dd98-4438-a845-4d2133edfb4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-pgq0kmvp\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-pgq0kmvp\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy (from clip==1.0)\n",
            "  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.16.0+cu118)\n",
            "Collecting wcwidth<0.3.0,>=0.2.12 (from ftfy->clip==1.0)\n",
            "  Downloading wcwidth-0.2.12-py2.py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369500 sha256=fcba6d013d13fb1aac04dc2f75d4a9b290b92ed66230619c814d577ab67cc617\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-43pg49mk/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: wcwidth, ftfy, clip\n",
            "  Attempting uninstall: wcwidth\n",
            "    Found existing installation: wcwidth 0.2.10\n",
            "    Uninstalling wcwidth-0.2.10:\n",
            "      Successfully uninstalled wcwidth-0.2.10\n",
            "Successfully installed clip-1.0 ftfy-6.1.3 wcwidth-0.2.12\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "wcwidth"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydrive in /usr/local/lib/python3.10/dist-packages (1.3.1)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.10/dist-packages (from pydrive) (2.84.0)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pydrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.10/dist-packages (from pydrive) (6.0.1)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.2->pydrive) (0.22.0)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.2->pydrive) (2.17.3)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.2->pydrive) (0.1.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.2->pydrive) (2.11.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.2->pydrive) (4.1.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client>=4.0.0->pydrive) (0.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from oauth2client>=4.0.0->pydrive) (0.3.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from oauth2client>=4.0.0->pydrive) (4.9)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from oauth2client>=4.0.0->pydrive) (1.16.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.2->pydrive) (1.61.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.2->pydrive) (3.20.3)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.2->pydrive) (2.31.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=1.2->pydrive) (5.3.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.15.0->google-api-python-client>=1.2->pydrive) (3.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.2->pydrive) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.2->pydrive) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.2->pydrive) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.2->pydrive) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "#@title Install\n",
        "!pip install transformers\n",
        "! pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install pydrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqE3Fj5-uYSR"
      },
      "outputs": [],
      "source": [
        "#@title Drive Downloader\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "download_with_pydrive = True #@param {type:\"boolean\"}\n",
        "\n",
        "class Downloader(object):\n",
        "    def __init__(self, use_pydrive):\n",
        "        self.use_pydrive = use_pydrive\n",
        "\n",
        "        if self.use_pydrive:\n",
        "            self.authenticate()\n",
        "\n",
        "    def authenticate(self):\n",
        "        auth.authenticate_user()\n",
        "        gauth = GoogleAuth()\n",
        "        gauth.credentials = GoogleCredentials.get_application_default()\n",
        "        self.drive = GoogleDrive(gauth)\n",
        "\n",
        "    def download_file(self, file_id, file_dst):\n",
        "        if self.use_pydrive:\n",
        "            downloaded = self.drive.CreateFile({'id':file_id})\n",
        "            downloaded.FetchMetadata(fetch_all=True)\n",
        "            downloaded.GetContentFile(file_dst)\n",
        "        else:\n",
        "            !gdown --id $file_id -O $file_dst\n",
        "\n",
        "downloader = Downloader(download_with_pydrive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OArDkm_24w4L"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "\n",
        "import clip\n",
        "import os\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as nnf\n",
        "import sys\n",
        "from typing import Tuple, List, Union, Optional\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm, trange\n",
        "from google.colab import files\n",
        "import skimage.io as io\n",
        "import PIL.Image\n",
        "from IPython.display import Image\n",
        "from enum import Enum\n",
        "\n",
        "\n",
        "\n",
        "N = type(None)\n",
        "V = np.array\n",
        "ARRAY = np.ndarray\n",
        "ARRAYS = Union[Tuple[ARRAY, ...], List[ARRAY]]\n",
        "VS = Union[Tuple[V, ...], List[V]]\n",
        "VN = Union[V, N]\n",
        "VNS = Union[VS, N]\n",
        "T = torch.Tensor\n",
        "TS = Union[Tuple[T, ...], List[T]]\n",
        "TN = Optional[T]\n",
        "TNS = Union[Tuple[TN, ...], List[TN]]\n",
        "TSN = Optional[TS]\n",
        "TA = Union[T, ARRAY]\n",
        "\n",
        "\n",
        "D = torch.device\n",
        "CPU = torch.device('cpu')\n",
        "\n",
        "\n",
        "def get_device(device_id: int) -> D:\n",
        "    if not torch.cuda.is_available():\n",
        "        return CPU\n",
        "    device_id = min(torch.cuda.device_count() - 1, device_id)\n",
        "    return torch.device(f'cuda:{device_id}')\n",
        "\n",
        "\n",
        "CUDA = get_device\n",
        "\n",
        "current_directory = os.getcwd()\n",
        "save_path = os.path.join(os.path.dirname(current_directory), \"pretrained_models\")\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "model_path = os.path.join(save_path, 'model_wieghts.pt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ClW2ebek8DK"
      },
      "outputs": [],
      "source": [
        "#@title Model\n",
        "\n",
        "\n",
        "class MappingType(Enum):\n",
        "    MLP = 'mlp'\n",
        "    Transformer = 'transformer'\n",
        "\n",
        "\n",
        "class MlpTransformer(nn.Module):\n",
        "     def __init__(self, in_dim, h_dim, out_d: Optional[int] = None, act=nnf.relu, dropout=0.):\n",
        "         super().__init__()\n",
        "         out_d = out_d if out_d is not None else in_dim\n",
        "         self.fc1 = nn.Linear(in_dim, h_dim)\n",
        "         self.act = act\n",
        "         self.fc2 = nn.Linear(h_dim, out_d)\n",
        "         self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "     def forward(self, x):\n",
        "         x = self.fc1(x)\n",
        "         x = self.act(x)\n",
        "         x = self.dropout(x)\n",
        "         x = self.fc2(x)\n",
        "         x = self.dropout(x)\n",
        "         return x\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.model(x)\n",
        "\n",
        "    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh):\n",
        "        super(MLP, self).__init__()\n",
        "        layers = []\n",
        "        for i in range(len(sizes) - 1):\n",
        "            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n",
        "            if i < len(sizes) - 2:\n",
        "                layers.append(act())\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, dim_self, dim_ref, num_heads, bias=True, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim_self // num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "        self.to_queries = nn.Linear(dim_self, dim_self, bias=bias)\n",
        "        self.to_keys_values = nn.Linear(dim_ref, dim_self * 2, bias=bias)\n",
        "        self.project = nn.Linear(dim_self, dim_self)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, y=None, mask=None):\n",
        "        y = y if y is not None else x\n",
        "        b, n, c = x.shape\n",
        "        _, m, d = y.shape\n",
        "        # b n h dh\n",
        "        queries = self.to_queries(x).reshape(b, n, self.num_heads, c // self.num_heads)\n",
        "        # b m 2 h dh\n",
        "        keys_values = self.to_keys_values(y).reshape(b, m, 2, self.num_heads, c // self.num_heads)\n",
        "        keys, values = keys_values[:, :, 0], keys_values[:, :, 1]\n",
        "        attention = torch.einsum('bnhd,bmhd->bnmh', queries, keys) * self.scale\n",
        "        if mask is not None:\n",
        "            if mask.dim() == 2:\n",
        "                mask = mask.unsqueeze(1)\n",
        "            attention = attention.masked_fill(mask.unsqueeze(3), float(\"-inf\"))\n",
        "        attention = attention.softmax(dim=2)\n",
        "        out = torch.einsum('bnmh,bmhd->bnhd', attention, values).reshape(b, n, c)\n",
        "        out = self.project(out)\n",
        "        return out, attention\n",
        "\n",
        "\n",
        "class TransformerLayer(nn.Module):\n",
        "\n",
        "    def forward_with_attention(self, x, y=None, mask=None):\n",
        "        x_, attention = self.attn(self.norm1(x), y, mask)\n",
        "        x = x + x_\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x, attention\n",
        "\n",
        "    def forward(self, x, y=None, mask=None):\n",
        "        x = x + self.attn(self.norm1(x), y, mask)[0]\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "    def __init__(self, dim_self, dim_ref, num_heads, mlp_ratio=4., bias=False, dropout=0., act=nnf.relu,\n",
        "                 norm_layer: nn.Module = nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim_self)\n",
        "        self.attn = MultiHeadAttention(dim_self, dim_ref, num_heads, bias=bias, dropout=dropout)\n",
        "        self.norm2 = norm_layer(dim_self)\n",
        "        self.mlp = MlpTransformer(dim_self, int(dim_self * mlp_ratio), act=act, dropout=dropout)\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "\n",
        "    def forward_with_attention(self, x, y=None, mask=None):\n",
        "        attentions = []\n",
        "        for layer in self.layers:\n",
        "            x, att = layer.forward_with_attention(x, y, mask)\n",
        "            attentions.append(att)\n",
        "        return x, attentions\n",
        "\n",
        "    def forward(self, x, y=None, mask=None):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            if i % 2 == 0 and self.enc_dec: # cross\n",
        "                x = layer(x, y)\n",
        "            elif self.enc_dec:  # self\n",
        "                x = layer(x, x, mask)\n",
        "            else:  # self or cross\n",
        "                x = layer(x, y, mask)\n",
        "        return x\n",
        "\n",
        "    def __init__(self, dim_self: int, num_heads: int, num_layers: int, dim_ref: Optional[int] = None,\n",
        "                 mlp_ratio: float = 2., act=nnf.relu, norm_layer: nn.Module = nn.LayerNorm, enc_dec: bool = False):\n",
        "        super(Transformer, self).__init__()\n",
        "        dim_ref = dim_ref if dim_ref is not None else dim_self\n",
        "        self.enc_dec = enc_dec\n",
        "        if enc_dec:\n",
        "            num_layers = num_layers * 2\n",
        "        layers = []\n",
        "        for i in range(num_layers):\n",
        "            if i % 2 == 0 and enc_dec:  # cross\n",
        "                layers.append(TransformerLayer(dim_self, dim_ref, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n",
        "            elif enc_dec:  # self\n",
        "                layers.append(TransformerLayer(dim_self, dim_self, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n",
        "            else:  # self or cross\n",
        "                layers.append(TransformerLayer(dim_self, dim_ref, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "\n",
        "\n",
        "class TransformerMapper(nn.Module):\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x).view(x.shape[0], self.clip_length, -1)\n",
        "        prefix = self.prefix_const.unsqueeze(0).expand(x.shape[0], *self.prefix_const.shape)\n",
        "        prefix = torch.cat((x, prefix), dim=1)\n",
        "        out = self.transformer(prefix)[:, self.clip_length:]\n",
        "        return out\n",
        "\n",
        "    def __init__(self, dim_clip: int, dim_embedding: int, prefix_length: int, clip_length: int, num_layers: int = 8):\n",
        "        super(TransformerMapper, self).__init__()\n",
        "        self.clip_length = clip_length\n",
        "        self.transformer = Transformer(dim_embedding, 8, num_layers)\n",
        "        self.linear = nn.Linear(dim_clip, clip_length * dim_embedding)\n",
        "        self.prefix_const = nn.Parameter(torch.randn(prefix_length, dim_embedding), requires_grad=True)\n",
        "\n",
        "\n",
        "class ClipCaptionModel(nn.Module):\n",
        "\n",
        "    def get_dummy_token(self, batch_size: int, device: torch.device) -> torch.Tensor:\n",
        "        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n",
        "\n",
        "    def forward(self, tokens: torch.Tensor, prefix: torch.Tensor, mask: Optional[torch.Tensor] = None,\n",
        "                labels: Optional[torch.Tensor] = None):\n",
        "        embedding_text = self.gpt.transformer.wte(tokens)\n",
        "        prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n",
        "        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n",
        "        if labels is not None:\n",
        "            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n",
        "            labels = torch.cat((dummy_token, tokens), dim=1)\n",
        "        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)\n",
        "        return out\n",
        "\n",
        "    def __init__(self, prefix_length: int, clip_length: Optional[int] = None, prefix_size: int = 512,\n",
        "                 num_layers: int = 8, mapping_type: MappingType = MappingType.MLP):\n",
        "        super(ClipCaptionModel, self).__init__()\n",
        "        self.prefix_length = prefix_length\n",
        "        self.gpt = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n",
        "        if mapping_type == MappingType.MLP:\n",
        "            self.clip_project = MLP((prefix_size, (self.gpt_embedding_size * prefix_length) // 2,\n",
        "                                     self.gpt_embedding_size * prefix_length))\n",
        "        else:\n",
        "            self.clip_project = TransformerMapper(prefix_size, self.gpt_embedding_size, prefix_length,\n",
        "                                                                     clip_length, num_layers)\n",
        "\n",
        "\n",
        "class ClipCaptionPrefix(ClipCaptionModel):\n",
        "\n",
        "    def parameters(self, recurse: bool = True):\n",
        "        return self.clip_project.parameters()\n",
        "\n",
        "    def train(self, mode: bool = True):\n",
        "        super(ClipCaptionPrefix, self).train(mode)\n",
        "        self.gpt.eval()\n",
        "        return self"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "V7xocT3TUgey"
      },
      "outputs": [],
      "source": [
        "#@title Caption prediction\n",
        "\n",
        "def generate_beam(model, tokenizer, beam_size: int = 5, prompt=None, embed=None,\n",
        "                  entry_length=67, temperature=1., stop_token: str = '.'):\n",
        "\n",
        "    model.eval()\n",
        "    stop_token_index = tokenizer.encode(stop_token)[0]\n",
        "    tokens = None\n",
        "    scores = None\n",
        "    device = next(model.parameters()).device\n",
        "    seq_lengths = torch.ones(beam_size, device=device)\n",
        "    is_stopped = torch.zeros(beam_size, device=device, dtype=torch.bool)\n",
        "    with torch.no_grad():\n",
        "        if embed is not None:\n",
        "            generated = embed\n",
        "        else:\n",
        "            if tokens is None:\n",
        "                tokens = torch.tensor(tokenizer.encode(prompt))\n",
        "                tokens = tokens.unsqueeze(0).to(device)\n",
        "                generated = model.gpt.transformer.wte(tokens)\n",
        "        for i in range(entry_length):\n",
        "            outputs = model.gpt(inputs_embeds=generated)\n",
        "            logits = outputs.logits\n",
        "            logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
        "            logits = logits.softmax(-1).log()\n",
        "            if scores is None:\n",
        "                scores, next_tokens = logits.topk(beam_size, -1)\n",
        "                generated = generated.expand(beam_size, *generated.shape[1:])\n",
        "                next_tokens, scores = next_tokens.permute(1, 0), scores.squeeze(0)\n",
        "                if tokens is None:\n",
        "                    tokens = next_tokens\n",
        "                else:\n",
        "                    tokens = tokens.expand(beam_size, *tokens.shape[1:])\n",
        "                    tokens = torch.cat((tokens, next_tokens), dim=1)\n",
        "            else:\n",
        "                logits[is_stopped] = -float(np.inf)\n",
        "                logits[is_stopped, 0] = 0\n",
        "                scores_sum = scores[:, None] + logits\n",
        "                seq_lengths[~is_stopped] += 1\n",
        "                scores_sum_average = scores_sum / seq_lengths[:, None]\n",
        "                scores_sum_average, next_tokens = scores_sum_average.view(-1).topk(beam_size, -1)\n",
        "                next_tokens_source = next_tokens // scores_sum.shape[1]\n",
        "                seq_lengths = seq_lengths[next_tokens_source]\n",
        "                next_tokens = next_tokens % scores_sum.shape[1]\n",
        "                next_tokens = next_tokens.unsqueeze(1)\n",
        "                tokens = tokens[next_tokens_source]\n",
        "                tokens = torch.cat((tokens, next_tokens), dim=1)\n",
        "                generated = generated[next_tokens_source]\n",
        "                scores = scores_sum_average * seq_lengths\n",
        "                is_stopped = is_stopped[next_tokens_source]\n",
        "            next_token_embed = model.gpt.transformer.wte(next_tokens.squeeze()).view(generated.shape[0], 1, -1)\n",
        "            generated = torch.cat((generated, next_token_embed), dim=1)\n",
        "            is_stopped = is_stopped + next_tokens.eq(stop_token_index).squeeze()\n",
        "            if is_stopped.all():\n",
        "                break\n",
        "    scores = scores / seq_lengths\n",
        "    output_list = tokens.cpu().numpy()\n",
        "    output_texts = [tokenizer.decode(output[:int(length)]) for output, length in zip(output_list, seq_lengths)]\n",
        "    order = scores.argsort(descending=True)\n",
        "    output_texts = [output_texts[i] for i in order]\n",
        "    return output_texts\n",
        "\n",
        "\n",
        "def generate2(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        tokens=None,\n",
        "        prompt=None,\n",
        "        embed=None,\n",
        "        entry_count=1,\n",
        "        entry_length=67,  # maximum number of words\n",
        "        top_p=0.8,\n",
        "        temperature=1.,\n",
        "        stop_token: str = '.',\n",
        "):\n",
        "    model.eval()\n",
        "    generated_num = 0\n",
        "    generated_list = []\n",
        "    stop_token_index = tokenizer.encode(stop_token)[0]\n",
        "    filter_value = -float(\"Inf\")\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for entry_idx in trange(entry_count):\n",
        "            if embed is not None:\n",
        "                generated = embed\n",
        "            else:\n",
        "                if tokens is None:\n",
        "                    tokens = torch.tensor(tokenizer.encode(prompt))\n",
        "                    tokens = tokens.unsqueeze(0).to(device)\n",
        "\n",
        "                generated = model.gpt.transformer.wte(tokens)\n",
        "\n",
        "            for i in range(entry_length):\n",
        "\n",
        "                outputs = model.gpt(inputs_embeds=generated)\n",
        "                logits = outputs.logits\n",
        "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
        "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "                cumulative_probs = torch.cumsum(nnf.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "                sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
        "                                                    ..., :-1\n",
        "                                                    ].clone()\n",
        "                sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "                logits[:, indices_to_remove] = filter_value\n",
        "                next_token = torch.argmax(logits, -1).unsqueeze(0)\n",
        "                next_token_embed = model.gpt.transformer.wte(next_token)\n",
        "                if tokens is None:\n",
        "                    tokens = next_token\n",
        "                else:\n",
        "                    tokens = torch.cat((tokens, next_token), dim=1)\n",
        "                generated = torch.cat((generated, next_token_embed), dim=1)\n",
        "                if stop_token_index == next_token.item():\n",
        "                    break\n",
        "\n",
        "            output_list = list(tokens.squeeze().cpu().numpy())\n",
        "            output_text = tokenizer.decode(output_list)\n",
        "            generated_list.append(output_text)\n",
        "\n",
        "    return generated_list[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xE-uUStuv1Nl"
      },
      "outputs": [],
      "source": [
        "#@title Download pretrained model over COCO\n",
        "\n",
        "\n",
        "pretrained_model = 'COCO'\n",
        "\n",
        "\n",
        "downloader.download_file(\"1GYPToCqFREwi285wPLhuVExlz7DDUDfJ\", model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7lCgFHSgr_ny"
      },
      "outputs": [],
      "source": [
        "#@title GPU/CPU\n",
        "\n",
        "\n",
        "is_gpu = True #@param {type:\"boolean\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6bi_2zQ3QD57"
      },
      "outputs": [],
      "source": [
        "#@title CLIP model + GPT2 tokenizer\n",
        "\n",
        "device = CUDA(0) if is_gpu else \"cpu\"\n",
        "clip_model, preprocess = clip.load(\"RN50x4\", device=device, jit=False)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "glBzYsgIwhwF"
      },
      "outputs": [],
      "source": [
        "#@title Load model weights\n",
        "\n",
        "\n",
        "prefix_length = 40\n",
        "\n",
        "model = ClipCaptionPrefix(prefix_length, clip_length=40, prefix_size=640,\n",
        "                                  num_layers=8, mapping_type='transformer')\n",
        "model.load_state_dict(torch.load(model_path, map_location=CPU))\n",
        "\n",
        "model = model.eval()\n",
        "device = CUDA(0) if is_gpu else \"cpu\"\n",
        "model = model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "m5jPDsEA5Kub"
      },
      "outputs": [],
      "source": [
        "#@title Upload Image\n",
        "\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "if not uploaded:\n",
        "  UPLOADED_FILE = ''\n",
        "elif len(uploaded) == 1:\n",
        "  UPLOADED_FILE = list(uploaded.keys())[0]\n",
        "else:\n",
        "  raise AssertionError('Please upload one image at a time')\n",
        "\n",
        "print(UPLOADED_FILE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pohtQ8AfWNk_"
      },
      "outputs": [],
      "source": [
        "#@title Or download random samples form COCO test set (Karpathy et al. split)\n",
        "\n",
        "IMAGE_NAME = '334321'  # @param ['562207', '579664', '060623', '165547', '334321', '483108', '386164', '354533']\n",
        "\n",
        "name_ = \"COCO_val2014_000000\" + IMAGE_NAME + \".jpg\"\n",
        "images_path = os.path.join(os.path.dirname(current_directory), \"images\")\n",
        "os.makedirs(images_path, exist_ok=True)\n",
        "UPLOADED_FILE = os.path.join(images_path, name_)\n",
        "\n",
        "if not os.path.isfile(UPLOADED_FILE):\n",
        "  download_path = os.path.join(images_path, \"images.zip\")\n",
        "  downloader.download_file(\"1l6J9WFYxpF-1HFr3A5Oq1eoObTxzbPgs\", download_path)\n",
        "\n",
        "  !unzip {download_path} -d {images_path}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xQC2FQPtKLbz"
      },
      "outputs": [],
      "source": [
        "#@title Inference\n",
        "use_beam_search = True #@param {type:\"boolean\"}\n",
        "\n",
        "image = io.imread(UPLOADED_FILE)\n",
        "pil_image = PIL.Image.fromarray(image)\n",
        "#pil_img = Image(filename=UPLOADED_FILE)\n",
        "display(pil_image)\n",
        "\n",
        "image = preprocess(pil_image).unsqueeze(0).to(device)\n",
        "with torch.no_grad():\n",
        "    prefix = clip_model.encode_image(image).to(device, dtype=torch.float32)\n",
        "    prefix = prefix / prefix.norm(2, -1).item()\n",
        "    prefix_embed = model.clip_project(prefix).reshape(1, prefix_length, -1)\n",
        "if use_beam_search:\n",
        "    generated_text_prefix = generate_beam(model, tokenizer, embed=prefix_embed)[0]\n",
        "else:\n",
        "    generated_text_prefix = generate2(model, tokenizer, embed=prefix_embed)\n",
        "\n",
        "\n",
        "print('\\n')\n",
        "print(generated_text_prefix)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "stem_cell": {
      "cell_type": "raw",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}