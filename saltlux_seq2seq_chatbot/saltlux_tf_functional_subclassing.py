# -*- coding: utf-8 -*-
"""saltlux_tf_functional_subclassing.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nXWQmseUeQkTnFlSq5SQkwycsF5VK8xo
"""

import tensorflow as tf
from tensorflow import keras
import numpy as np

# sequential API


vocab_size = 100
emb_size = 256
hidden_dim = 1024
output_dimension = 10

# seq
model = keras.Sequential()
model.add(keras.Input(shape = (5,)))
model.add(keras.layers.Embedding(vocab_size, emb_size))
model.add(keras.layers.Lambda(lambda x: tf.reduce_mean(x, axis = 1)))
model.add(keras.layers.Dense(hidden_dim, activation = "relu"))
model.add(keras.layers.Dense(output_dimension, activation = "softmax"))


# model.compile(loss = "categorical_crossentropy", optimizer = "adam", metrics = ['acc'])

# model.fit(inputs, target, epochs=20, batch_size=20)

# Functional API

inputs = keras.Input(shape=(5,))
emb_output = keras.layers.Embedding(vocab_size, emb_size)(inputs)
mean_layer = tf.reduce_mean(emb_output, axis=1)
dense = keras.layers.Dense(hidden_dim, activation = "relu")(mean_layer)
outputs = keras.layers.Dense(output_dimension, activation = "softmax")(dense)

model = keras.Model(inputs, outputs, name = "Functional_API")

vocab_size = 100
emb_size = 256
hidden_dim = 1024
output_dimension = 10

class CustomModel(keras.Model):
  def __init__(self, vocab_size, emb_size, hidden_dim, output_dimension):
    super(CustomModel, self).__init__()
    self.embedding = keras.layers.Embedding(vocab_size, emb_size)
    self.dense = keras.layers.Dense(hidden_dim, activation = 'relu')
    self.outputs = keras.layers.Dense(output_dimension, activation = 'softmax')

  def call(self, inputs):
    x = self.embedding(inputs)
    x = tf.reduce_mean(x, axis=1)
    x = self.dense(x)
    x = self.outputs(x)

    return x

