{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from preprocess import *\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './data_in/ChatBotData.csv_short'\n",
    "VOCAB_PATH = './data_in/vocabulary.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    data_df = pd.read_csv(path, header = 0)\n",
    "    question, answer = list(data_df['Q']), list(data_df['A'])\n",
    "    return question, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: \n",
      " ['가끔 궁금해', '가끔 뭐하는지 궁금해', '가끔은 혼자인게 좋다', '가난한 자의 설움', '가만 있어도 땀난다', '가상화폐 쫄딱 망함', '가스불 켜고 나갔어', '가스불 켜놓고 나온거 같아', '가스비 너무 많이 나왔다.', '가스비 비싼데 감기 걸리겠어', '남자친구 교회 데려가고 싶어', '남자친구 또 운동 갔어', '남자친구 생일인데 뭘 줄까', '남자친구 승진 선물로 뭐가 좋을까?', '남자친구 오늘 따라 훈훈해 보인다', '남자친구 오늘 좀 질린다.', '남자친구가 나 안 믿어줘', '남자친구가 너무 바빠', '남자친구가 너무 운동만 해', '남자친구가 너무 잘생겼어']\n",
      "outputs: \n",
      " ['그 사람도 그럴 거예요.', '그 사람도 그럴 거예요.', '혼자를 즐기세요.', '돈은 다시 들어올 거예요.', '땀을 식혀주세요.', '어서 잊고 새출발 하세요.', '빨리 집에 돌아가서 끄고 나오세요.', '빨리 집에 돌아가서 끄고 나오세요.', '다음 달에는 더 절약해봐요.', '따뜻하게 사세요!', '마음을 열 때까지 설득해보세요.', '운동을 함께 해보세요.', '평소에 필요한 것 생각해보세요.', '평소에 필요했던 게 좋을 것 같아요.', '전생에 나라를 구하셨나요.', '결단은 빠를수록 좋아요.', '거짓말 적당히 하세요.', '너무 집착하지 마세요.', '운동을 함께 해보세요.', '전생에 나라를 구하셨나요.']\n",
      "inputs + outputs: \n",
      " ['가끔 궁금해', '가끔 뭐하는지 궁금해', '가끔은 혼자인게 좋다', '가난한 자의 설움', '가만 있어도 땀난다', '가상화폐 쫄딱 망함', '가스불 켜고 나갔어', '가스불 켜놓고 나온거 같아', '가스비 너무 많이 나왔다.', '가스비 비싼데 감기 걸리겠어', '남자친구 교회 데려가고 싶어', '남자친구 또 운동 갔어', '남자친구 생일인데 뭘 줄까', '남자친구 승진 선물로 뭐가 좋을까?', '남자친구 오늘 따라 훈훈해 보인다', '남자친구 오늘 좀 질린다.', '남자친구가 나 안 믿어줘', '남자친구가 너무 바빠', '남자친구가 너무 운동만 해', '남자친구가 너무 잘생겼어', '그 사람도 그럴 거예요.', '그 사람도 그럴 거예요.', '혼자를 즐기세요.', '돈은 다시 들어올 거예요.', '땀을 식혀주세요.', '어서 잊고 새출발 하세요.', '빨리 집에 돌아가서 끄고 나오세요.', '빨리 집에 돌아가서 끄고 나오세요.', '다음 달에는 더 절약해봐요.', '따뜻하게 사세요!', '마음을 열 때까지 설득해보세요.', '운동을 함께 해보세요.', '평소에 필요한 것 생각해보세요.', '평소에 필요했던 게 좋을 것 같아요.', '전생에 나라를 구하셨나요.', '결단은 빠를수록 좋아요.', '거짓말 적당히 하세요.', '너무 집착하지 마세요.', '운동을 함께 해보세요.', '전생에 나라를 구하셨나요.']\n"
     ]
    }
   ],
   "source": [
    "inputs, outputs = load_data(PATH) # inputs = question, outputs = answer\n",
    "print('inputs: \\n', inputs)\n",
    "print('outputs: \\n', outputs)\n",
    "\n",
    "data = []\n",
    "data.extend(inputs)\n",
    "data.extend(outputs)\n",
    "print(\"inputs + outputs: \\n\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:03<00:00, 11.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['가끔 궁금해', '가끔 뭐 하는지 궁금해', '가끔 은 혼자 인게 좋다', '가난한 자의 설움', '가만있어도 땀 난 다', '가상 화폐 쫄 딱 망함', '가스 불 켜고 나갔어', '가스 불 켜놓고 나온거 같아', '가스 비 너무 많이 나왔다 .', '가스 비비 싼데 감기 걸리겠어', '남자친구 교회 데려가고싶어', '남자친구 또 운동 갔어', '남자친구 생일 인데 뭘 줄까', '남자친구 승진 선물 로 뭐 가 좋을까 ?', '남자친구 오늘 따라 훈훈해 보인다', '남자친구 오늘 좀질 린다 .', '남자친구 가나안 믿어줘', '남자친구 가 너무 바빠', '남자친구 가 너 무운 동 만해', '남자친구 가 너무 잘생겼어', '그 사람 도 그럴거예요 .', '그 사람 도 그럴거예요 .', '혼자 를 즐기세요 .', '돈 은 다시 들어올거예요 .', '땀 을 식혀주세요 .', '어서 잊고 새 출발 하세요 .', '빨리 집 에 돌아가서 끄고나오세요 .', '빨리 집 에 돌아가서 끄고나오세요 .', '다음 달 에는 더 절약 해봐요 .', '따뜻하게 사세요 !', '마음 을 열 때 까지 설득 해보세요 .', '운동 을 함께 해보세요 .', '평소 에 필요한것 생각 해보세요 .', '평소 에 필요했던게 좋을것 같아요 .', '전생 에 나라 를 구 하셨나요 .', '결단 은 빠를수록 좋아요 .', '거짓말 적당히하세요 .', '너 무집 착하지 마세요 .', '운동 을 함께 해보세요 .', '전생 에 나라 를 구 하셨나요 .']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def prepro_like_morphlized(data):\n",
    "    morph_analyzer = Okt()\n",
    "    result_data = []\n",
    "    for seq in tqdm(data):\n",
    "        morphlized_seq = \" \".join(morph_analyzer.morphs(seq.replace(' ', '')))\n",
    "        result_data.append(morphlized_seq)\n",
    "    return result_data\n",
    "\n",
    "morphized_data = prepro_like_morphlized(data)\n",
    "print(morphized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############### Checking code\n",
    "# data = []\n",
    "# data.extend(inputs)\n",
    "# data.extend(outputs)\n",
    "\n",
    "# data\n",
    "# data2 = inputs + outputs\n",
    "# data == data2 # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['가끔', '궁금해', '가끔', '뭐하는지', '궁금해', '가끔은', '혼자인게', '좋다', '가난한', '자의', '설움', '가만', '있어도', '땀난다', '가상화폐', '쫄딱', '망함', '가스불', '켜고', '나갔어', '가스불', '켜놓고', '나온거', '같아', '가스비', '너무', '많이', '나왔다', '가스비', '비싼데', '감기', '걸리겠어', '남자친구', '교회', '데려가고', '싶어', '남자친구', '또', '운동', '갔어', '남자친구', '생일인데', '뭘', '줄까', '남자친구', '승진', '선물로', '뭐가', '좋을까', '남자친구', '오늘', '따라', '훈훈해', '보인다', '남자친구', '오늘', '좀', '질린다', '남자친구가', '나', '안', '믿어줘', '남자친구가', '너무', '바빠', '남자친구가', '너무', '운동만', '해', '남자친구가', '너무', '잘생겼어', '그', '사람도', '그럴', '거예요', '그', '사람도', '그럴', '거예요', '혼자를', '즐기세요', '돈은', '다시', '들어올', '거예요', '땀을', '식혀주세요', '어서', '잊고', '새출발', '하세요', '빨리', '집에', '돌아가서', '끄고', '나오세요', '빨리', '집에', '돌아가서', '끄고', '나오세요', '다음', '달에는', '더', '절약해봐요', '따뜻하게', '사세요', '마음을', '열', '때까지', '설득해보세요', '운동을', '함께', '해보세요', '평소에', '필요한', '것', '생각해보세요', '평소에', '필요했던', '게', '좋을', '것', '같아요', '전생에', '나라를', '구하셨나요', '결단은', '빠를수록', '좋아요', '거짓말', '적당히', '하세요', '너무', '집착하지', '마세요', '운동을', '함께', '해보세요', '전생에', '나라를', '구하셨나요']\n",
      "143\n",
      "['설움', '남자친구', '그', '따뜻하게', '운동만', '가스불', '비싼데', '다음', '마세요', '믿어줘', '새출발', '게', '생각해보세요', '가끔', '바빠', '좋아요', '좋다', '쫄딱', '같아', '구하셨나요', '갔어', '적당히', '돈은', '자의', '많이', '것', '망함', '절약해봐요', '해보세요', '잘생겼어', '싶어', '감기', '데려가고', '가끔은', '보인다', '돌아가서', '함께', '거예요', '운동', '필요한', '뭘', '혼자를', '더', '빠를수록', '교회', '사람도', '하세요', '들어올', '잊고', '집에', '설득해보세요', '필요했던', '나오세요', '혼자인게', '좀', '어서', '좋을까', '그럴', '열', '가만', '걸리겠어', '달에는', '나온거', '집착하지', '해', '전생에', '나', '오늘', '가스비', '즐기세요', '나갔어', '가난한', '같아요', '뭐가', '선물로', '궁금해', '나왔다', '나라를', '생일인데', '따라', '사세요', '식혀주세요', '또', '땀을', '뭐하는지', '가상화폐', '켜고', '켜놓고', '승진', '좋을', '질린다', '때까지', '운동을', '땀난다', '줄까', '결단은', '평소에', '끄고', '있어도', '마음을', '안', '너무', '남자친구가', '다시', '거짓말', '빨리', '훈훈해']\n",
      "107\n"
     ]
    }
   ],
   "source": [
    "## make words list\n",
    "FILTERS = \"([~.,!?\\\"':;)(])\"\n",
    "CHANGE_FILTER = re.compile(FILTERS)\n",
    "\n",
    "def data_tokenizer(data):\n",
    "    words = []\n",
    "    for sentence in data:\n",
    "        sentence = re.sub(CHANGE_FILTER, \"\", sentence)\n",
    "        for word in sentence.split():\n",
    "            words.append(word)\n",
    "\n",
    "    # return [word for word in words if word] #Textbook\n",
    "    return words # same with Textbook \n",
    "\n",
    "\n",
    "word_list = data_tokenizer(data)\n",
    "# word_list = data_tokenizer(morphized_data)\n",
    "\n",
    "print(word_list)\n",
    "print(len(word_list))\n",
    "\n",
    "## set으로 변경\n",
    "words = list(set(word_list))\n",
    "print(words)\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n"
     ]
    }
   ],
   "source": [
    "PAD = \"<PAD>\"\n",
    "STD = \"<SOS>\"\n",
    "END = \"<END>\"\n",
    "UNK = \"<UNK>\"\n",
    "\n",
    "PAD_INDEX = 0\n",
    "STD_INDEX = 1\n",
    "END_INDEX = 2\n",
    "UNK_INDEX = 3\n",
    "\n",
    "\n",
    "MARKER = [PAD, STD, END, UNK]\n",
    "words[:0] = MARKER\n",
    "print(len(words))\n",
    "\n",
    "## save words file\n",
    "# write() argument must be str, not list\n",
    "with open(VOCAB_PATH, 'w', encoding='utf-8') as vocabulary_file:\n",
    "    for word in words:\n",
    "        vocabulary_file.write(word + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 파일이 존재하면 여기에서\n",
    "# 그 파일을 불러서 리스트배열에 넣어 준다.\n",
    "vocabulary_list = []\n",
    "if os.path.exists(VOCAB_PATH):\n",
    "    with open(VOCAB_PATH, 'r', encoding='utf-8') as vocabulary_file:\n",
    "        for line in vocabulary_file:\n",
    "            vocabulary_list.append(line.strip()) # ['<PAD>\\n', '<SOS>\\n', '<END>\\n', ... ,\n",
    "\n",
    "# vocabulary_list = words\n",
    "# print(vocabulary_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char2idx: \n",
      "\n",
      "{'<PAD>': 4, '<SOS>': 5, '<END>': 6, '<UNK>': 7, '설움': 8, '남자친구': 9, '그': 10, '따뜻하게': 11, '운동만': 12, '가스불': 13, '비싼데': 14, '다음': 15, '마세요': 16, '믿어줘': 17, '새출발': 18, '게': 19, '생각해보세요': 20, '가끔': 21, '바빠': 22, '좋아요': 23, '좋다': 24, '쫄딱': 25, '같아': 26, '구하셨나요': 27, '갔어': 28, '적당히': 29, '돈은': 30, '자의': 31, '많이': 32, '것': 33, '망함': 34, '절약해봐요': 35, '해보세요': 36, '잘생겼어': 37, '싶어': 38, '감기': 39, '데려가고': 40, '가끔은': 41, '보인다': 42, '돌아가서': 43, '함께': 44, '거예요': 45, '운동': 46, '필요한': 47, '뭘': 48, '혼자를': 49, '더': 50, '빠를수록': 51, '교회': 52, '사람도': 53, '하세요': 54, '들어올': 55, '잊고': 56, '집에': 57, '설득해보세요': 58, '필요했던': 59, '나오세요': 60, '혼자인게': 61, '좀': 62, '어서': 63, '좋을까': 64, '그럴': 65, '열': 66, '가만': 67, '걸리겠어': 68, '달에는': 69, '나온거': 70, '집착하지': 71, '해': 72, '전생에': 73, '나': 74, '오늘': 75, '가스비': 76, '즐기세요': 77, '나갔어': 78, '가난한': 79, '같아요': 80, '뭐가': 81, '선물로': 82, '궁금해': 83, '나왔다': 84, '나라를': 85, '생일인데': 86, '따라': 87, '사세요': 88, '식혀주세요': 89, '또': 90, '땀을': 91, '뭐하는지': 92, '가상화폐': 93, '켜고': 94, '켜놓고': 95, '승진': 96, '좋을': 97, '질린다': 98, '때까지': 99, '운동을': 100, '땀난다': 101, '줄까': 102, '결단은': 103, '평소에': 104, '끄고': 105, '있어도': 106, '마음을': 107, '안': 108, '너무': 109, '남자친구가': 110, '다시': 111, '거짓말': 112, '빨리': 113, '훈훈해': 114}\n",
      "idx2char: \n",
      "\n",
      "{0: '<PAD>', 1: '<SOS>', 2: '<END>', 3: '<UNK>', 4: '<PAD>', 5: '<SOS>', 6: '<END>', 7: '<UNK>', 8: '설움', 9: '남자친구', 10: '그', 11: '따뜻하게', 12: '운동만', 13: '가스불', 14: '비싼데', 15: '다음', 16: '마세요', 17: '믿어줘', 18: '새출발', 19: '게', 20: '생각해보세요', 21: '가끔', 22: '바빠', 23: '좋아요', 24: '좋다', 25: '쫄딱', 26: '같아', 27: '구하셨나요', 28: '갔어', 29: '적당히', 30: '돈은', 31: '자의', 32: '많이', 33: '것', 34: '망함', 35: '절약해봐요', 36: '해보세요', 37: '잘생겼어', 38: '싶어', 39: '감기', 40: '데려가고', 41: '가끔은', 42: '보인다', 43: '돌아가서', 44: '함께', 45: '거예요', 46: '운동', 47: '필요한', 48: '뭘', 49: '혼자를', 50: '더', 51: '빠를수록', 52: '교회', 53: '사람도', 54: '하세요', 55: '들어올', 56: '잊고', 57: '집에', 58: '설득해보세요', 59: '필요했던', 60: '나오세요', 61: '혼자인게', 62: '좀', 63: '어서', 64: '좋을까', 65: '그럴', 66: '열', 67: '가만', 68: '걸리겠어', 69: '달에는', 70: '나온거', 71: '집착하지', 72: '해', 73: '전생에', 74: '나', 75: '오늘', 76: '가스비', 77: '즐기세요', 78: '나갔어', 79: '가난한', 80: '같아요', 81: '뭐가', 82: '선물로', 83: '궁금해', 84: '나왔다', 85: '나라를', 86: '생일인데', 87: '따라', 88: '사세요', 89: '식혀주세요', 90: '또', 91: '땀을', 92: '뭐하는지', 93: '가상화폐', 94: '켜고', 95: '켜놓고', 96: '승진', 97: '좋을', 98: '질린다', 99: '때까지', 100: '운동을', 101: '땀난다', 102: '줄까', 103: '결단은', 104: '평소에', 105: '끄고', 106: '있어도', 107: '마음을', 108: '안', 109: '너무', 110: '남자친구가', 111: '다시', 112: '거짓말', 113: '빨리', 114: '훈훈해'}\n"
     ]
    }
   ],
   "source": [
    "def make_vocabulary(vocabulary_list):\n",
    "    char2idx = {char: idx for idx, char in enumerate(vocabulary_list)}\n",
    "    idx2char = {idx: char for idx, char in enumerate(vocabulary_list)}\n",
    "\n",
    "    return char2idx, idx2char\n",
    "\n",
    "char2idx, idx2char = make_vocabulary(vocabulary_list)\n",
    "print(\"char2idx: \\n\")\n",
    "print(char2idx)\n",
    "print(\"idx2char: \\n\")\n",
    "print(idx2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing 함수\n",
    "FILTERS = \"([~.,!?\\\"':;)(])\"\n",
    "CHANGE_FILTER = re.compile(FILTERS)\n",
    "\n",
    "PAD = \"<PAD>\"\n",
    "STD = \"<SOS>\"\n",
    "END = \"<END>\"\n",
    "UNK = \"<UNK>\"\n",
    "\n",
    "PAD_INDEX = 0\n",
    "STD_INDEX = 1\n",
    "END_INDEX = 2\n",
    "UNK_INDEX = 3\n",
    "\n",
    "MARKER = [PAD, STD, END, UNK]\n",
    "\n",
    "\n",
    "def prepro_like_morphlized(data):\n",
    "    morph_analyzer = Okt()\n",
    "    result_data = list()\n",
    "    for seq in tqdm(data):\n",
    "        morphlized_seq = \" \".join(morph_analyzer.morphs(seq.replace(' ', '')))\n",
    "        result_data.append(morphlized_seq)\n",
    "\n",
    "    return result_data\n",
    "\n",
    "def data_tokenizer(data):\n",
    "    # 토크나이징 해서 담을 배열 생성\n",
    "    words = []\n",
    "    for sentence in data:\n",
    "        # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
    "        # 위 필터와 같은 값들을 정규화 표현식을\n",
    "        # 통해서 모두 \"\" 으로 변환 해주는 부분이다.\n",
    "        sentence = re.sub(CHANGE_FILTER, \"\", sentence)\n",
    "        for word in sentence.split():\n",
    "            words.append(word)\n",
    "    # 토그나이징과 정규표현식을 통해 만들어진\n",
    "    # 값들을 넘겨 준다.\n",
    "    return [word for word in words if word]\n",
    "\n",
    "def make_vocabulary(vocabulary_list):\n",
    "    # 리스트를 키가 단어이고 값이 인덱스인\n",
    "    # 딕셔너리를 만든다.\n",
    "    char2idx = {char: idx for idx, char in enumerate(vocabulary_list)}\n",
    "    # 리스트를 키가 인덱스이고 값이 단어인\n",
    "    # 딕셔너리를 만든다.\n",
    "    idx2char = {idx: char for idx, char in enumerate(vocabulary_list)}\n",
    "    # 두개의 딕셔너리를 넘겨 준다.\n",
    "    return char2idx, idx2char\n",
    "\n",
    "######################\n",
    "def load_vocabulary(path, vocab_path, tokenize_as_morph=False):\n",
    "    vocabulary_list = []\n",
    "\n",
    "    if not os.path.exists(vocab_path):\n",
    "        if (os.path.exists(path)):\n",
    "            data_df = pd.read_csv(path, encoding='utf-8')\n",
    "            question, answer = list(data_df['Q']), list(data_df['A'])\n",
    "            if tokenize_as_morph:  # 형태소에 따른 토크나이져 처리\n",
    "                question = prepro_like_morphlized(question)\n",
    "                answer = prepro_like_morphlized(answer)\n",
    "            data = []\n",
    "            \n",
    "            data.extend(question)\n",
    "            data.extend(answer) # same question + answer\n",
    "\n",
    "            # 토큰나이져 처리 하는 부분이다.\n",
    "            words = data_tokenizer(data)\n",
    "            # set해주고 이것들을 리스트로 만들어 준다.\n",
    "            words = list(set(words))\n",
    "            # PAD = \"<PADDING>\"\n",
    "            # STD = \"<START>\"\n",
    "            # END = \"<END>\"\n",
    "            # UNK = \"<UNKNWON>\"\n",
    "            words[:0] = MARKER\n",
    "            # 사전 파일을 만들어 넣는다.\n",
    "            with open(vocab_path, 'w', encoding='utf-8') as vocabulary_file:\n",
    "                for word in words:\n",
    "                    vocabulary_file.write(word + '\\n')\n",
    "\n",
    "    # 사전 파일이 존재하면 여기에서\n",
    "    # 그 파일을 불러서 리스트배열에 넣어 준다.\n",
    "    with open(vocab_path, 'r', encoding='utf-8') as vocabulary_file:\n",
    "        for line in vocabulary_file:\n",
    "            vocabulary_list.append(line.strip()) # ['<PAD>\\n', '<SOS>\\n', '<END>\\n', ... ,\n",
    "\n",
    "    # 딕셔너리 구조로 만든다.\n",
    "    char2idx, idx2char = make_vocabulary(vocabulary_list)\n",
    "\n",
    "    # (예) 단어: 인덱스 , 인덱스: 단어)\n",
    "    return char2idx, idx2char, len(char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char2idx:\n",
      "\n",
      "{'<PAD>': 0, '<SOS>': 1, '<END>': 2, '<UNK>': 3, '설움': 4, '남자친구': 5, '그': 6, '따뜻하게': 7, '운동만': 8, '가스불': 9, '비싼데': 10, '다음': 11, '마세요': 12, '믿어줘': 13, '새출발': 14, '게': 15, '생각해보세요': 16, '가끔': 17, '바빠': 18, '좋아요': 19, '좋다': 20, '쫄딱': 21, '같아': 22, '구하셨나요': 23, '갔어': 24, '적당히': 25, '돈은': 26, '자의': 27, '많이': 28, '것': 29, '망함': 30, '절약해봐요': 31, '해보세요': 32, '잘생겼어': 33, '싶어': 34, '감기': 35, '데려가고': 36, '가끔은': 37, '보인다': 38, '돌아가서': 39, '함께': 40, '거예요': 41, '운동': 42, '필요한': 43, '뭘': 44, '혼자를': 45, '더': 46, '빠를수록': 47, '교회': 48, '사람도': 49, '하세요': 50, '들어올': 51, '잊고': 52, '집에': 53, '설득해보세요': 54, '필요했던': 55, '나오세요': 56, '혼자인게': 57, '좀': 58, '어서': 59, '좋을까': 60, '그럴': 61, '열': 62, '가만': 63, '걸리겠어': 64, '달에는': 65, '나온거': 66, '집착하지': 67, '해': 68, '전생에': 69, '나': 70, '오늘': 71, '가스비': 72, '즐기세요': 73, '나갔어': 74, '가난한': 75, '같아요': 76, '뭐가': 77, '선물로': 78, '궁금해': 79, '나왔다': 80, '나라를': 81, '생일인데': 82, '따라': 83, '사세요': 84, '식혀주세요': 85, '또': 86, '땀을': 87, '뭐하는지': 88, '가상화폐': 89, '켜고': 90, '켜놓고': 91, '승진': 92, '좋을': 93, '질린다': 94, '때까지': 95, '운동을': 96, '땀난다': 97, '줄까': 98, '결단은': 99, '평소에': 100, '끄고': 101, '있어도': 102, '마음을': 103, '안': 104, '너무': 105, '남자친구가': 106, '다시': 107, '거짓말': 108, '빨리': 109, '훈훈해': 110}\n",
      "idx2char:\n",
      "\n",
      "{0: '<PAD>', 1: '<SOS>', 2: '<END>', 3: '<UNK>', 4: '설움', 5: '남자친구', 6: '그', 7: '따뜻하게', 8: '운동만', 9: '가스불', 10: '비싼데', 11: '다음', 12: '마세요', 13: '믿어줘', 14: '새출발', 15: '게', 16: '생각해보세요', 17: '가끔', 18: '바빠', 19: '좋아요', 20: '좋다', 21: '쫄딱', 22: '같아', 23: '구하셨나요', 24: '갔어', 25: '적당히', 26: '돈은', 27: '자의', 28: '많이', 29: '것', 30: '망함', 31: '절약해봐요', 32: '해보세요', 33: '잘생겼어', 34: '싶어', 35: '감기', 36: '데려가고', 37: '가끔은', 38: '보인다', 39: '돌아가서', 40: '함께', 41: '거예요', 42: '운동', 43: '필요한', 44: '뭘', 45: '혼자를', 46: '더', 47: '빠를수록', 48: '교회', 49: '사람도', 50: '하세요', 51: '들어올', 52: '잊고', 53: '집에', 54: '설득해보세요', 55: '필요했던', 56: '나오세요', 57: '혼자인게', 58: '좀', 59: '어서', 60: '좋을까', 61: '그럴', 62: '열', 63: '가만', 64: '걸리겠어', 65: '달에는', 66: '나온거', 67: '집착하지', 68: '해', 69: '전생에', 70: '나', 71: '오늘', 72: '가스비', 73: '즐기세요', 74: '나갔어', 75: '가난한', 76: '같아요', 77: '뭐가', 78: '선물로', 79: '궁금해', 80: '나왔다', 81: '나라를', 82: '생일인데', 83: '따라', 84: '사세요', 85: '식혀주세요', 86: '또', 87: '땀을', 88: '뭐하는지', 89: '가상화폐', 90: '켜고', 91: '켜놓고', 92: '승진', 93: '좋을', 94: '질린다', 95: '때까지', 96: '운동을', 97: '땀난다', 98: '줄까', 99: '결단은', 100: '평소에', 101: '끄고', 102: '있어도', 103: '마음을', 104: '안', 105: '너무', 106: '남자친구가', 107: '다시', 108: '거짓말', 109: '빨리', 110: '훈훈해'}\n",
      "\n",
      "vocab_size =  111\n"
     ]
    }
   ],
   "source": [
    "PATH = './data_in/ChatBotData.csv_short'\n",
    "VOCAB_PATH = './data_in/vocabulary.txt'\n",
    "\n",
    "char2idx, idx2char, vocab_size = load_vocabulary(PATH, VOCAB_PATH)\n",
    "\n",
    "print(\"char2idx:\\n\")\n",
    "print(char2idx)\n",
    "\n",
    "print(\"idx2char:\\n\")\n",
    "print(idx2char)\n",
    "\n",
    "print()\n",
    "print(\"vocab_size = \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## encoder processing\n",
    "## question == inputs\n",
    "MAX_SEQUENCE = 25\n",
    "\n",
    "# sequences_input_index = []\n",
    "# sequences_length = []\n",
    "\n",
    "# for sequence in inputs:\n",
    "#     sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
    "\n",
    "#     sequence_index = []\n",
    "#     for word in sequence.split():\n",
    "#         if char2idx.get(word) is not None:\n",
    "#             sequence_index.extend([char2idx[word]])\n",
    "#         else:\n",
    "#             sequence_index.extend([char2idx[UNK]])\n",
    "\n",
    "#     if len(sequence_index) > MAX_SEQUENCE:\n",
    "#         sequence_index = sequence_index[:MAX_SEQUENCE]\n",
    "\n",
    "#     sequences_length.append(len(sequence_index))\n",
    "#     sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [char2idx[PAD]]\n",
    "#     sequences_input_index.append(sequence_index)\n",
    "\n",
    "def enc_processing(value, dictionary, tokenize_as_morph=False):\n",
    "    sequences_input_index = []\n",
    "    sequences_length = []\n",
    "\n",
    "    if tokenize_as_morph:\n",
    "        value = prepro_like_morphlized(value)\n",
    "\n",
    "    for sequence in value:\n",
    "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
    "\n",
    "        sequence_index = []\n",
    "        for word in sequence.split():\n",
    "            if dictionary.get(word) is not None:\n",
    "                sequence_index.extend([dictionary[word]])\n",
    "            else:\n",
    "                sequence_index.extend([dictionary[UNK]])\n",
    "\n",
    "        if len(sequence_index) > MAX_SEQUENCE:\n",
    "            sequence_index = sequence_index[:MAX_SEQUENCE]\n",
    "\n",
    "        sequences_length.append(len(sequence_index))\n",
    "        sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]\n",
    "        sequences_input_index.append(sequence_index)\n",
    "\n",
    "    return np.asarray(sequences_input_index), sequences_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 17  79   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [ 17  88  79   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [ 37  57  20   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [ 75  27   4   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [ 63 102  97   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [ 89  21  30   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  9  90  74   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  9  91  66  22   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [ 72 105  28  80   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [ 72  10  35  64   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  5  48  36  34   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  5  86  42  24   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  5  82  44  98   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  5  92  78  77  60   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  5  71  83 110  38   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  5  71  58  94   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [106  70 104  13   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [106 105  18   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [106 105   8  68   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [106 105  33   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]]\n",
      "[2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 4, 4, 3, 4, 3]\n"
     ]
    }
   ],
   "source": [
    "## question == inputs\n",
    "index_inputs, input_seq_len = enc_processing(inputs, char2idx, tokenize_as_morph=False)\n",
    "\n",
    "print(index_inputs)\n",
    "print(input_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## decoder output processing\n",
    "def dec_output_processing(value, dictionary, tokenize_as_morph=False):\n",
    "    sequences_output_index = []\n",
    "    sequences_length = []\n",
    "\n",
    "    if tokenize_as_morph:\n",
    "        value = prepro_like_morphlized(value)\n",
    "\n",
    "    for sequence in value:\n",
    "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
    "\n",
    "        sequence_index = []\n",
    "        # sequence_index = [dictionary[STD]] + \\\n",
    "        #             [dictionary[word] if word in dictionary else dictionary[UNK] for word in sequence.split()]\n",
    "        for word in sequence.split():\n",
    "            if dictionary.get(word) is not None:\n",
    "                sequence_index.extend([dictionary[word]])\n",
    "            else:\n",
    "                sequence_index.extend([dictionary[UNK]])\n",
    "\n",
    "        sequence_index = [dictionary[STD]] + sequence_index\n",
    "\n",
    "        if len(sequence_index) > MAX_SEQUENCE:\n",
    "            sequence_index = sequence_index[:MAX_SEQUENCE]\n",
    "\n",
    "        sequences_length.append(len(sequence_index))\n",
    "        sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]\n",
    "        sequences_output_index.append(sequence_index)\n",
    "\n",
    "    return np.asarray(sequences_output_index), sequences_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1   6  49  61  41   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  1   6  49  61  41   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  1  45  73   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  1  26 107  51  41   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  1  87  85   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  1  59  52  14  50   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  1 109  53  39 101  56   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  1 109  53  39 101  56   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  1  11  65  46  31   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  1   7  84   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  1 103  62  95  54   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  1  96  40  32   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  1 100  43  29  16   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  1 100  55  15  93  29  76   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  1  69  81  23   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  1  99  47  19   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  1 108  25  50   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  1 105  67  12   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  1  96  40  32   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  1  69  81  23   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]]\n",
      "[5, 5, 3, 5, 3, 5, 6, 6, 5, 3, 5, 4, 5, 7, 4, 4, 4, 4, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "index_outputs, output_seq_len = dec_output_processing(outputs, char2idx, tokenize_as_morph=False)\n",
    "print(index_outputs)\n",
    "print(output_seq_len)\n",
    "\n",
    "# print(index_outputs2 == index_outputs) # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## decoder target processing\n",
    "def dec_target_processing(value, dictionary, tokenize_as_morph=False):\n",
    "    sequences_target_index = []\n",
    "   \n",
    "\n",
    "    if tokenize_as_morph:\n",
    "        value = prepro_like_morphlized(value)\n",
    "\n",
    "    for sequence in value:\n",
    "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
    "\n",
    "        sequence_index = []\n",
    "        # sequence_index = [dictionary[STD]] + \\\n",
    "        #             [dictionary[word] if word in dictionary else dictionary[UNK] for word in sequence.split()]\n",
    "        for word in sequence.split():\n",
    "            if dictionary.get(word) is not None:\n",
    "                sequence_index.extend([dictionary[word]])\n",
    "            else:\n",
    "                sequence_index.extend([dictionary[UNK]])\n",
    "\n",
    "        \n",
    "        if len(sequence_index) >= MAX_SEQUENCE:\n",
    "            sequence_index = sequence_index[:MAX_SEQUENCE-1] + [dictionary[END]]\n",
    "        else:\n",
    "            sequence_index += [dictionary[END]]\n",
    "\n",
    "        # sequences_length.append(len(sequence_index))\n",
    "        sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]\n",
    "        sequences_target_index.append(sequence_index)\n",
    "\n",
    "    return np.asarray(sequences_target_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_targets = dec_target_processing(outputs, char2idx, tokenize_as_morph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가상화폐 쫄딱 망함\n",
      "[89 21 30  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0]\n",
      "어서 잊고 새출발 하세요.\n",
      "[ 1 59 52 14 50  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0]\n",
      "[59 52 14 50  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0]\n"
     ]
    }
   ],
   "source": [
    "index = 5\n",
    "print(inputs[index])\n",
    "print(index_inputs[index])\n",
    "\n",
    "print(outputs[index])\n",
    "print(index_outputs[index])\n",
    "\n",
    "print(index_targets[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_configs = {}\n",
    "data_configs['char2idx'] = char2idx\n",
    "data_configs['idx2char'] = idx2char\n",
    "data_configs['vocab_size'] = vocab_size\n",
    "data_configs['pad_symbol'] = PAD\n",
    "data_configs['std_symbol'] = STD\n",
    "data_configs['end_symbol'] = END\n",
    "data_configs['unk_symbol'] = UNK\n",
    "\n",
    "DATA_IN_PATH = './data_in/'\n",
    "TRAIN_INPUTS = 'train_inputs.npy'\n",
    "TRAIN_OUTPUTS = 'train_outputs.npy'\n",
    "TRAIN_TARGETS = 'train_targets.npy'\n",
    "DATA_CONFIGS = 'data_configs.json'\n",
    "\n",
    "np.save(open(DATA_IN_PATH + TRAIN_INPUTS, 'wb'), index_inputs)\n",
    "np.save(open(DATA_IN_PATH + TRAIN_OUTPUTS , 'wb'), index_outputs)\n",
    "np.save(open(DATA_IN_PATH + TRAIN_TARGETS , 'wb'), index_targets)\n",
    "\n",
    "json.dump(data_configs, open(DATA_IN_PATH + DATA_CONFIGS, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<PAD>': 0, '<SOS>': 1, '<END>': 2, '<UNK>': 3, '설움': 4, '남자친구': 5, '그': 6, '따뜻하게': 7, '운동만': 8, '가스불': 9, '비싼데': 10, '다음': 11, '마세요': 12, '믿어줘': 13, '새출발': 14, '게': 15, '생각해보세요': 16, '가끔': 17, '바빠': 18, '좋아요': 19, '좋다': 20, '쫄딱': 21, '같아': 22, '구하셨나요': 23, '갔어': 24, '적당히': 25, '돈은': 26, '자의': 27, '많이': 28, '것': 29, '망함': 30, '절약해봐요': 31, '해보세요': 32, '잘생겼어': 33, '싶어': 34, '감기': 35, '데려가고': 36, '가끔은': 37, '보인다': 38, '돌아가서': 39, '함께': 40, '거예요': 41, '운동': 42, '필요한': 43, '뭘': 44, '혼자를': 45, '더': 46, '빠를수록': 47, '교회': 48, '사람도': 49, '하세요': 50, '들어올': 51, '잊고': 52, '집에': 53, '설득해보세요': 54, '필요했던': 55, '나오세요': 56, '혼자인게': 57, '좀': 58, '어서': 59, '좋을까': 60, '그럴': 61, '열': 62, '가만': 63, '걸리겠어': 64, '달에는': 65, '나온거': 66, '집착하지': 67, '해': 68, '전생에': 69, '나': 70, '오늘': 71, '가스비': 72, '즐기세요': 73, '나갔어': 74, '가난한': 75, '같아요': 76, '뭐가': 77, '선물로': 78, '궁금해': 79, '나왔다': 80, '나라를': 81, '생일인데': 82, '따라': 83, '사세요': 84, '식혀주세요': 85, '또': 86, '땀을': 87, '뭐하는지': 88, '가상화폐': 89, '켜고': 90, '켜놓고': 91, '승진': 92, '좋을': 93, '질린다': 94, '때까지': 95, '운동을': 96, '땀난다': 97, '줄까': 98, '결단은': 99, '평소에': 100, '끄고': 101, '있어도': 102, '마음을': 103, '안': 104, '너무': 105, '남자친구가': 106, '다시': 107, '거짓말': 108, '빨리': 109, '훈훈해': 110}\n",
      "{0: '<PAD>', 1: '<SOS>', 2: '<END>', 3: '<UNK>', 4: '설움', 5: '남자친구', 6: '그', 7: '따뜻하게', 8: '운동만', 9: '가스불', 10: '비싼데', 11: '다음', 12: '마세요', 13: '믿어줘', 14: '새출발', 15: '게', 16: '생각해보세요', 17: '가끔', 18: '바빠', 19: '좋아요', 20: '좋다', 21: '쫄딱', 22: '같아', 23: '구하셨나요', 24: '갔어', 25: '적당히', 26: '돈은', 27: '자의', 28: '많이', 29: '것', 30: '망함', 31: '절약해봐요', 32: '해보세요', 33: '잘생겼어', 34: '싶어', 35: '감기', 36: '데려가고', 37: '가끔은', 38: '보인다', 39: '돌아가서', 40: '함께', 41: '거예요', 42: '운동', 43: '필요한', 44: '뭘', 45: '혼자를', 46: '더', 47: '빠를수록', 48: '교회', 49: '사람도', 50: '하세요', 51: '들어올', 52: '잊고', 53: '집에', 54: '설득해보세요', 55: '필요했던', 56: '나오세요', 57: '혼자인게', 58: '좀', 59: '어서', 60: '좋을까', 61: '그럴', 62: '열', 63: '가만', 64: '걸리겠어', 65: '달에는', 66: '나온거', 67: '집착하지', 68: '해', 69: '전생에', 70: '나', 71: '오늘', 72: '가스비', 73: '즐기세요', 74: '나갔어', 75: '가난한', 76: '같아요', 77: '뭐가', 78: '선물로', 79: '궁금해', 80: '나왔다', 81: '나라를', 82: '생일인데', 83: '따라', 84: '사세요', 85: '식혀주세요', 86: '또', 87: '땀을', 88: '뭐하는지', 89: '가상화폐', 90: '켜고', 91: '켜놓고', 92: '승진', 93: '좋을', 94: '질린다', 95: '때까지', 96: '운동을', 97: '땀난다', 98: '줄까', 99: '결단은', 100: '평소에', 101: '끄고', 102: '있어도', 103: '마음을', 104: '안', 105: '너무', 106: '남자친구가', 107: '다시', 108: '거짓말', 109: '빨리', 110: '훈훈해'}\n",
      "111\n"
     ]
    }
   ],
   "source": [
    "print(data_configs['char2idx'])\n",
    "print(data_configs['idx2char'])\n",
    "print(data_configs['vocab_size'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
